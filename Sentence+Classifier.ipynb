{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# COGS 109: (Data Modeling & Analysis)",
    "# Final Project: Sentence Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributors:\n",
    "* **Philip Leo Pascual**\n",
    "* **Benjamin Isip**\n",
    "* **Gustav Santo-Tomas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "**With natural language processing as our motivation, we came to decide on a topic that will be using text-based data for our analysis. In pursuing a more novel topic, we chose to implement a sentence classifier as oppose to a document classifier. This in part lead us to use the [Sentence Classification Dataset](http://archive.ics.uci.edu/ml/datasets/Sentence+Classification) from UCI's machine learning database.**\n",
    "\n",
    "**By implementing a sentence classifier we would hope to get some insight to how features of a sentence contributes to its own \"meaning/purpose\". This analysis will look into methods that one can use to classify sentences within the context of research papers, to identify key parts of a given text. We would hope that this project brings up the awareness of how aspects of sentences affect their intended usage.**\n",
    "\n",
    "\n",
    "## Dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Pre-Processing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['### abstract ###',\n",
       " 'Fitness functions based on test cases are very common in Genetic Programming (GP)',\n",
       " 'This process can be assimilated to a learning task, with the inference of models from a limited number of samples',\n",
       " 'This paper is an investigation on two methods to improve generalization in GP-based learning: 1) the selection of the best-of-run individuals using a three data sets methodology, and 2) the application of parsimony pressure in order to reduce the complexity of the solutions',\n",
       " 'Results using GP in a binary classification setup show that while the accuracy on the test sets is preserved, with less variances compared to baseline results, the mean tree size obtained with the tested methods is significantly reduced']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unlabled text file\n",
    "# Note: Move an unlabeled text file into the same directory\n",
    "#       as this jupyter notebook\n",
    "unlab_file = open(\"unlabeled.txt\", \"r\")\n",
    "unlab_text = unlab_file.read()\n",
    "unlab_list = unlab_text.split('\\n')\n",
    "\n",
    "unlab_file.close()\n",
    "\n",
    "# Note: Sentences are separated by '\\n'\n",
    "\n",
    "# List of sentences\n",
    "unlab_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### abstract ###\\nMISC Although the Internet AS-level topology has been extensively studied over the past few years, little is known about the details of the AS taxonomy\\nMISC An AS \"node\" can represent a wide variety of organizations, e g , large ISP, or small private business, university, with vastly different network characteristics, external connectivity patterns, network growth tendencies, and other properties that we can hardly neglect while working on veracious Internet representations in simulation environments\\nAIMX In this paper, we introduce a radically new approach based on machine learning techniques to map all the ASes in the Internet into a natural AS taxonomy\\nOWNX We successfully classify ~95.3\\\\% of ASes with expected accuracy of ~78.1\\\\%\\nOWNX We release to the community the AS-level topology dataset augmented with: 1) the AS taxonomy information and 2) the set of AS attributes we used to classify ASes\\nOWNX We believe that this dataset will serve as an invaluable addition to further understanding of the structure and evolution of the Internet\\n### introduction ###\\nMISC The rapid expansion of the Internet in the last two decades has produced a large-scale system of thousands of diverse, independently managed networks that collectively provide global connectivity across a wide spectrum of geopolitical environments\\nMISC From 1997 to 2005 the number of globally routable AS identifiers has increased from less than 2,000 to more than 20,000, exerting significant pressure on interdomain routing as well as other functional and structural parts of the Internet\\nMISC This impressive growth has resulted in a heterogenous and highly complex system that challenges accurate and realistic modeling of the Internet infrastructure\\nMISC In particular, the AS-level topology is an intermix of networks owned and operated by many different organizations, e g , backbone providers, regional providers, access providers, universities and private companies\\nMISC Statistical information that faithfully characterizes different AS types is on the critical path toward understanding the structure of the Internet, as well as for modeling its topology and growth\\nMISC In topology modeling, knowledge of AS types is mandatory for augmenting synthetically constructed or measured AS topologies with realistic intra-AS and inter-AS router-level topologies\\nMISC For example, we expect the network of a dual-homed university to be drastically different from that of a dual-homed small company\\nMISC The university will likely contain dozens of internal routers, thousands of hosts, and many other network elements (switches, servers, firewalls)\\nMISC On the other hand, the small company will most probably have a single router and a simple network topology\\nMISC Since there is such a diversity among different network types, we cannot accurately augment the AS-level topology with appropriate router-level topologies if we cannot characterize the composing ASes\\nMISC Moreover, annotating the ASes in the AS topology with their types is a prerequisite for modeling the evolution of the Internet, since different types of ASes exhibit different growth patterns\\nMISC For example, Internet Service Providers (ISP) grow by attracting new customers and by engaging in business agreements with other ISPs\\nMISC On the other hand, small companies that connect to the Internet through one or few ISPs do not grow significantly over time\\nMISC Thus, categorizing different types of ASes in the Internet is necessary to identify network evolution patterns and develop accurate evolution models\\nMISC An AS taxonomy is also necessary for mapping IP addresses to different types of users\\nMISC For example, in traffic analysis studies its often required to distinguish between packets that come from home and business users\\nMISC Given an AS taxonomy, its possible to realize this goal by checking the type of AS that originates the prefix in which an IP address lies\\nAIMX In this work, we introduce a radically new approach based on machine learning to construct a representative AS taxonomy\\nOWNX We develop an algorithm to classify ASes based on empirically observed differences between AS characteristics\\nBASE We use a large set of data from the Internet Routing Registries~(IRR)~ CITATION  and from RouteViews~ CITATION  to identify intrinsic differences between ASes of different types\\nOWNX Then, we employ a novel machine learning technique to build a classification algorithm that exploits these differences to classify ASes into six representative classes that reflect ASes with different network properties and infrastructures\\nOWNX We derive macroscopic statistics on the different types of ASes in the Internet and validate our results using a sample of~1200 manually identified AS types\\nOWNX Our validation demonstrates that our classification algorithm achieves high accuracy:~78 1\\\\% of the examined classifications were correct\\nOWNX Finally, we make our results and our classifier publicly available to promote further research and understanding of the Internet\\'s structure and evolution\\nOWNX In Section~ we start with a brief discussion of related work\\nOWNX Section~ describes the data we used, and in Section~ we specify the set of AS classes we use in our experiments\\nOWNX Section~ introduces our classification approach and results\\nOWNX We validate them in Section~ and conclude in Section~\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labeled text file\n",
    "# Note: I chose a random labeled text file\n",
    "text_file = open(\"arxiv_annotate1_13_3.txt\", \"r\")\n",
    "body_text= text_file.read()\n",
    "\n",
    "# Note: Sentence's labels are found at the end of each sentence, \n",
    "#       hence we can separate them when we find a label\n",
    "\n",
    "text_file.close()\n",
    "body_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Label': 'MISC', 'Sentence': '# # # abstract # # #'},\n",
       " {'Label': 'MISC',\n",
       "  'Sentence': 'Although the Internet AS-level topology has been extensively studied over the past few years , little is known about the details of the AS taxonomy'},\n",
       " {'Label': 'AIMX',\n",
       "  'Sentence': \"An AS `` node '' can represent a wide variety of organizations , e g , large ISP , or small private business , university , with vastly different network characteristics , external connectivity patterns , network growth tendencies , and other properties that we can hardly neglect while working on veracious Internet representations in simulation environments\"},\n",
       " {'Label': 'OWNX',\n",
       "  'Sentence': 'In this paper , we introduce a radically new approach based on machine learning techniques to map all the ASes in the Internet into a natural AS taxonomy'},\n",
       " {'Label': 'OWNX',\n",
       "  'Sentence': 'We successfully classify ~95.3\\\\ % of ASes with expected accuracy of ~78.1\\\\ %'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############ Word Tokenization ############\n",
    "text_list = nltk.word_tokenize(body_text)\n",
    "\n",
    "############ Sentence Segmentation ############\n",
    "sents_list = []   # List of sentences from text \n",
    "temp_list = []    # Stores words temporary to form sentences\n",
    "\n",
    "# Iterates through the list of words to extract sents\n",
    "for word in text_list:\n",
    "    # If the current word is a label do the following\n",
    "    if re.fullmatch(r'(AIMX|OWNX|CONT|BASE|MISC)', word):\n",
    "        # Dictionary for each sentence\n",
    "        sents_temp = {}\n",
    "        sents_temp[\"Sentence\"] = \" \".join(temp_list)\n",
    "        sents_temp[\"Label\"] = word\n",
    "        sents_list.append(sents_temp)\n",
    "        \n",
    "        # Reset the list & increment count\n",
    "        temp_list = [] \n",
    "    else:\n",
    "        temp_list.append(word)\n",
    "\n",
    "# List of dictionaries where each entry represents a sentence & its label\n",
    "sents_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv_annotate1_13_3.txt: 34 sentences\n"
     ]
    }
   ],
   "source": [
    "# Number of sentences\n",
    "print(\"{}: {} sentences\".format(\"arxiv_annotate1_13_3.txt\", len(sents_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text files: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['arxiv_annotate10_7_3.txt',\n",
       " 'arxiv_annotate1_13_3.txt',\n",
       " 'arxiv_annotate2_66_3.txt',\n",
       " 'arxiv_annotate3_80_3.txt',\n",
       " 'arxiv_annotate4_168_3.txt',\n",
       " 'arxiv_annotate5_240_3.txt',\n",
       " 'arxiv_annotate6_52_3.txt',\n",
       " 'arxiv_annotate7_268_3.txt',\n",
       " 'arxiv_annotate8_81_3.txt',\n",
       " 'arxiv_annotate9_279_3.txt',\n",
       " 'jdm_annotate10_210_3.txt',\n",
       " 'jdm_annotate1_103_3.txt',\n",
       " 'jdm_annotate2_107_3.txt',\n",
       " 'jdm_annotate3_120_3.txt',\n",
       " 'jdm_annotate4_220_3.txt',\n",
       " 'jdm_annotate5_228_3.txt',\n",
       " 'jdm_annotate6_32_3.txt',\n",
       " 'jdm_annotate7_265_3.txt',\n",
       " 'jdm_annotate8_177_3.txt',\n",
       " 'jdm_annotate9_45_3.txt',\n",
       " 'plos_annotate10_1140_3.txt',\n",
       " 'plos_annotate1_6_3.txt',\n",
       " 'plos_annotate2_336_3.txt',\n",
       " 'plos_annotate3_798_3.txt',\n",
       " 'plos_annotate4_1052_3.txt',\n",
       " 'plos_annotate5_1375_3.txt',\n",
       " 'plos_annotate6_1032_3.txt',\n",
       " 'plos_annotate7_1233_3.txt',\n",
       " 'plos_annotate8_123_3.txt',\n",
       " 'plos_annotate9_1187_3.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################ Text File Extraction ################\n",
    "import os\n",
    "# Place jupyternotebook into 'labeled_articles' directory \n",
    "# Replace Pathway with your own\n",
    "root = \"C:/Users/Leo Pascual/Desktop/UCSD/Fall_Quarter_2017/Cogs_109/Final_Project/SentenceCorpus/labeled_articles\"\n",
    "textfiles = []\n",
    "\n",
    "# Directory that contains all the text files\n",
    "directory = os.listdir(root)\n",
    "\n",
    "# We only want to look at the third annotators files\n",
    "for file in directory:\n",
    "    if file.endswith(\"3.txt\"):\n",
    "        textfiles.append(file)\n",
    "\n",
    "print('Number of text files: {}'.format(len(textfiles)))\n",
    "\n",
    "# 30 annotated text files\n",
    "textfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Compiling the List of Stop-Words ################\n",
    "import itertools \n",
    "\n",
    "# NLTK's list of stop-words\n",
    "stop_words = list(set(nltk.corpus.stopwords.words(\"english\")))\n",
    "\n",
    "# Compile the list of stop-words we would want to filter out\n",
    "\n",
    "################ Text File Extraction ################\n",
    "# Note: Place the word_lists directory into the labeled_articles directory\n",
    "# Note: Remove the stop-words text file - we will be using nltk's stop-words list\n",
    "root = \"C:/Users/Leo Pascual/Desktop/UCSD/Fall_Quarter_2017/\" \\\n",
    "       \"Cogs_109/Final_Project/SentenceCorpus/labeled_articles/word_lists\"\n",
    "directory = os.listdir(root)\n",
    "\n",
    "# Extract the files we want\n",
    "stopwords_files = []\n",
    "for file in directory:\n",
    "    if file.endswith(\".txt\"):\n",
    "        stopwords_files.append(file)\n",
    "        \n",
    "# Opens each file \n",
    "textwrappers = []       # Stores all textwrappers before being read\n",
    "for file in stopwords_files:\n",
    "    textwrappers.append(open(root + \"/\" + file, 'r'))\n",
    "\n",
    "# Compile the list of stop-words for each text file(label)\n",
    "file_stopwords = []\n",
    "for wrapper in textwrappers:\n",
    "    body_text = wrapper.read()\n",
    "    file_stopwords.append(body_text.split(\"\\n\"))\n",
    "\n",
    "# list of stop-words that we would want to keep according to the dataset README\n",
    "dataset_stopwords = list(sorted(set(list(itertools.chain.from_iterable(file_stopwords)))))\n",
    "\n",
    "# Remove the wanted stop-words from NLTK's list\n",
    "for stopword in dataset_stopwords:\n",
    "    if stopword in stop_words:\n",
    "        stop_words.remove(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Label': 'OWNX', 'Sentence': '# # # abstract # # #'},\n",
       " {'Label': 'MISC',\n",
       "  'Sentence': 'the minimum description length principle online sequence estimation/prediction proper learning setup is studied'},\n",
       " {'Label': 'MISC',\n",
       "  'Sentence': 'if underlying model class is discrete , total expected square loss is particularly interesting performance measure : ( ) this quantity is finitely bounded , implying convergence probability one , ( b ) additionally specifies convergence speed'},\n",
       " {'Label': 'AIMX',\n",
       "  'Sentence': 'for mdl , general one only have loss bounds are finite exponentially larger bayes mixtures'},\n",
       " {'Label': 'OWNX',\n",
       "  'Sentence': 'we show this is even case model class contains only bernoulli distributions'},\n",
       " {'Label': 'OWNX',\n",
       "  'Sentence': 'we derive new upper bound on prediction error countable bernoulli classes'},\n",
       " {'Label': 'OWNX',\n",
       "  'Sentence': 'this implies small bound ( comparable to one bayes mixtures ) certain important model classes'},\n",
       " {'Label': 'MISC',\n",
       "  'Sentence': 'we discuss application to machine learning tasks classification hypothesis testing , generalization to countable classes iid models # # # introduction # # #'},\n",
       " {'Label': 'MISC',\n",
       "  'Sentence': \"`` bayes mixture '' , `` solomonoff induction '' , `` marginalization '' , these terms refer to central induction principle : obtain predictive distribution integrating product prior evidence model class\"},\n",
       " {'Label': 'MISC',\n",
       "  'Sentence': 'in many cases however , bayes mixture is computationally infeasible , even sophisticated approximation is expensive'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textwrappers = []       # Stores all textwrappers before being read\n",
    "textfile_tokens = []    # Stores each textfiles list of words\n",
    "\n",
    "# Opens all the files\n",
    "for file in textfiles:\n",
    "    textwrappers.append(open(file, 'r'))\n",
    "    \n",
    "############ Word Tokenization ############\n",
    "# Tokenizes each file \n",
    "for wrapper in textwrappers:\n",
    "    temp_list = []\n",
    "    body_text = wrapper.read()\n",
    "    temp_list = nltk.word_tokenize(body_text)\n",
    "    textfile_tokens.append(temp_list)\n",
    "\n",
    "    \n",
    "################ Sentence Segmentation ################\n",
    "sents_list = []   # List of sentences from text \n",
    "temp_list = []    # Stores words temporary to form sentences\n",
    "\n",
    "# Iterates through each list of tokens \n",
    "for text_list in textfile_tokens:\n",
    "    # Iterates through the list of words to extract sents\n",
    "    for word in text_list:\n",
    "        # If the current word: is a label do the following\n",
    "        if re.fullmatch(r'(AIMX|OWNX|CONT|BASE|MISC)', word):\n",
    "            # Dictionary for each sentence\n",
    "            sents_temp = {}\n",
    "            sents_temp[\"Sentence\"] = \" \".join(temp_list)\n",
    "            sents_temp[\"Label\"] = word\n",
    "            sents_list.append(sents_temp)\n",
    "\n",
    "            # Reset the list \n",
    "            temp_list = [] \n",
    "        else:\n",
    "            # Removes stop-words\n",
    "            if word not in stop_words:\n",
    "                # Forms the sentence until a label is found\n",
    "                temp_list.append(word.lower())\n",
    "\n",
    "# Close files\n",
    "for wrapper in textwrappers:\n",
    "    wrapper.close()\n",
    "    \n",
    "# List of sentences and their label\n",
    "sents_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 1040\n"
     ]
    }
   ],
   "source": [
    "# Number of labled sentences out of the 30 text files\n",
    "print('Number of training sentences: {}'.format(len(sents_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeled Text: Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>the description length symbol parameter symbol...</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>a prior weight may defined symbol</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>if string symbol is generated bernoulli distri...</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>that is , two-part complexity respect to berno...</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>many machine learning tasks are reduced to seq...</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>an important example is classification</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>the task classifying new instance symbol seen ...</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>typically ( instance , class ) pairs are iid</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>cumulative loss bounds prediction usually gene...</td>\n",
       "      <td>OWNX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>then we solve classification problems standard...</td>\n",
       "      <td>OWNX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>it is not obvious how proofs this paper condit...</td>\n",
       "      <td>OWNX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>our main tool obtaining results is kullback-le...</td>\n",
       "      <td>OWNX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>lemmata this quantity are stated section</td>\n",
       "      <td>OWNX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>section shows exponential error bound obtained...</td>\n",
       "      <td>OWNX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>in section , we give upper bound on instantane...</td>\n",
       "      <td>OWNX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>the latter bound is small eg certain condition...</td>\n",
       "      <td>OWNX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>section treats universal setup</td>\n",
       "      <td>OWNX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>finally , section we discuss results give conc...</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>although internet as-level topology extensivel...</td>\n",
       "      <td>MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>an as `` node '' represent wide variety organi...</td>\n",
       "      <td>AIMX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence Label\n",
       "50  the description length symbol parameter symbol...  MISC\n",
       "51                  a prior weight may defined symbol  MISC\n",
       "52  if string symbol is generated bernoulli distri...  MISC\n",
       "53  that is , two-part complexity respect to berno...  MISC\n",
       "54  many machine learning tasks are reduced to seq...  MISC\n",
       "55             an important example is classification  MISC\n",
       "56  the task classifying new instance symbol seen ...  MISC\n",
       "57       typically ( instance , class ) pairs are iid  MISC\n",
       "58  cumulative loss bounds prediction usually gene...  OWNX\n",
       "59  then we solve classification problems standard...  OWNX\n",
       "60  it is not obvious how proofs this paper condit...  OWNX\n",
       "61  our main tool obtaining results is kullback-le...  OWNX\n",
       "62           lemmata this quantity are stated section  OWNX\n",
       "63  section shows exponential error bound obtained...  OWNX\n",
       "64  in section , we give upper bound on instantane...  OWNX\n",
       "65  the latter bound is small eg certain condition...  OWNX\n",
       "66                     section treats universal setup  OWNX\n",
       "67  finally , section we discuss results give conc...  MISC\n",
       "68  although internet as-level topology extensivel...  MISC\n",
       "69  an as `` node '' represent wide variety organi...  AIMX"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Dictionary to Data Frame ###########\n",
    "# Training set\n",
    "df = pd.DataFrame.from_dict(sents_list, orient = 'columns')\n",
    "df = df[['Sentence', 'Label']]\n",
    "df.iloc[50:70,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlabeled Text: Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/Leo Pascual/Desktop/UCSD/Fall_Quarter_2017/Cogs_109/Final_Project/SentenceCorpus/labeled_articles/arxiv_unlabeled',\n",
       " 'C:/Users/Leo Pascual/Desktop/UCSD/Fall_Quarter_2017/Cogs_109/Final_Project/SentenceCorpus/labeled_articles/jdm_unlabeled',\n",
       " 'C:/Users/Leo Pascual/Desktop/UCSD/Fall_Quarter_2017/Cogs_109/Final_Project/SentenceCorpus/labeled_articles/plos_unlabeled']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################ Directory Extraction ################\n",
    "# Place the three (unlabeled file) directories within labeled_articles directory\n",
    "# Replace path with your own\n",
    "root = \"C:/Users/Leo Pascual/Desktop/UCSD/Fall_Quarter_2017/Cogs_109/Final_Project/SentenceCorpus/labeled_articles\"\n",
    "unlabeled_folders = [\"arxiv_unlabeled\", \"jdm_unlabeled\", \"plos_unlabeled\"]\n",
    "list_unlabeled = []\n",
    "\n",
    "# List of directory paths to a folder\n",
    "for file in unlabeled_folders:\n",
    "    list_unlabeled.append(root + \"/\" + file)\n",
    "\n",
    "list_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['### abstract ###',\n",
       " 'Fitness functions based on test cases are very common in Genetic Programming (GP)',\n",
       " 'This process can be assimilated to a learning task, with the inference of models from a limited number of samples',\n",
       " 'This paper is an investigation on two methods to improve generalization in GP-based learning: 1) the selection of the best-of-run individuals using a three data sets methodology, and 2) the application of parsimony pressure in order to reduce the complexity of the solutions',\n",
       " 'Results using GP in a binary classification setup show that while the accuracy on the test sets is preserved, with less variances compared to baseline results, the mean tree size obtained with the tested methods is significantly reduced',\n",
       " '### introduction ###',\n",
       " 'GP is particularly suited for problems that can be assimilated to learning tasks, with the minimization of the error between the obtained and desired outputs for a limited number of test cases -- the training data, using a ML terminology',\n",
       " 'Indeed, the classical GP examples of symbolic regression, boolean multiplexer and artificial ant  CITATION  are only simple instances of well-known learning problems (i e respectively regression, binary classification and reinforcement learning)',\n",
       " 'In the early years of GP, these problems were tackled using a single data set, reporting results on the same data set that was used to evaluate the fitnesses during the evolution',\n",
       " 'This was justifiable by the fact that these are toy problems used only to illustrate the potential of GP']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################ Text File Extraction ################\n",
    "unlabeled_textfiles = []    # List of (list of text files) for each folder\n",
    "for path in list_unlabeled:\n",
    "    temp_list = []          # Stores files\n",
    "    directory = os.listdir(path)\n",
    "    for file in directory:\n",
    "        temp_list.append(file)\n",
    "    unlabeled_textfiles.append(temp_list) \n",
    " \n",
    "################ Sentence segmentation ################\n",
    "total_sents = []            # List of (list of sentences) for each file \n",
    "for i, list_textfiles in enumerate(unlabeled_textfiles):\n",
    "    for file in list_textfiles:\n",
    "        unlab_file = open(list_unlabeled[i] + \"/\" + file, encoding=\"Latin-1\")\n",
    "        unlab_text = unlab_file.read()\n",
    "        total_sents.append(unlab_text.split('\\n'))  # Note: Each sentence is separated by a \"\\n\"\n",
    "\n",
    "total_sents[0][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Sentences Within Training & Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 1040\n",
      "Number of sents within our testing set: 37970\n"
     ]
    }
   ],
   "source": [
    "################ Dataset Size ################\n",
    "count = 0 # Counts the number of total sents within our dataset\n",
    "for file in total_sents:\n",
    "    for sents in file:\n",
    "        count += 1\n",
    "\n",
    "print('Number of training sentences: {}'.format(len(sents_list)))        \n",
    "print('Number of sents within our testing set: {}'.format(count))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "['### abstract ###', 'Fitness functions based on test cases are very common in Genetic Programming (GP)', 'This process can be assimilated to a learning task, with the inference of models from a limited number of samples', 'This paper is an investigation on two methods to improve generalization in GP-based learning: 1) the selection of the best-of-run individuals using a three data sets methodology, and 2) the application of parsimony pressure in order to reduce the complexity of the solutions', 'Results using GP in a binary classification setup show that while the accuracy on the test sets is preserved, with less variances compared to baseline results, the mean tree size obtained with the tested methods is significantly reduced']\n",
      "\n",
      "\n",
      "['### abstract ###', 'fitness functions based on test cases are very common in genetic programming (gp)', 'this process can be assimilated to a learning task, with the inference of models from a limited number of samples', 'this paper is an investigation on two methods to improve generalization in gp-based learning: 1) the selection of the best-of-run individuals using a three data sets methodology, and 2) the application of parsimony pressure in order to reduce the complexity of the solutions', 'results using gp in a binary classification setup show that while the accuracy on the test sets is preserved, with less variances compared to baseline results, the mean tree size obtained with the tested methods is significantly reduced']\n"
     ]
    }
   ],
   "source": [
    "################ Lowercase Conversion ################\n",
    "print(\"\".join(separator))\n",
    "print(total_sents[0][0:5])\n",
    "\n",
    "# Testing set\n",
    "new_total_sents = []\n",
    "for txt_file in total_sents:\n",
    "    for i, sent in enumerate(txt_file):\n",
    "        new_total_sents.append(sent.lower())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(new_total_sents[0:5])"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
